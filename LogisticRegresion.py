# Veri iÅŸleme ve gÃ¶rselleÅŸtirme kÃ¼tÃ¼phaneleri
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Makine Ã¶ÄŸrenmesi modÃ¼lleri
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# CSV dosyasÄ±nÄ± oku
df = pd.read_csv("diabetes_data_upload.csv")  # Dosya adÄ± farklÄ±ysa, dosyanÄ±zÄ±n adÄ±nÄ± buraya girin.
df.head()

# Veri setinin genel bilgilerini inceleyelim
df.info()

# Eksik verilerin sayÄ±sÄ±nÄ± gÃ¶relim
df.isnull().sum()

# 'Yes/No' gibi string ifadeleri sayÄ±sal verilere Ã§eviriyoruz
df = df.replace({'Yes': 1, 'No': 0})
df = df.replace({'Male': 1, 'Female': 0})
df = df.replace({'Positive': 1, 'Negative': 0})
df.head()

# Hedef sÃ¼tun: class (Diyabet var mÄ± yok mu?)
X = df.drop("class", axis=1)  # 'class' sÃ¼tununu Ã§Ä±karÄ±yoruz, geri kalan tÃ¼m sÃ¼tunlar X olacak
y = df["class"]               # 'class' sÃ¼tunu hedef deÄŸiÅŸken

# Ä°lk 5 satÄ±rÄ± kontrol edelim
X.head()

# Veriyi %80 eÄŸitim, %20 test olarak bÃ¶lÃ¼yoruz
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# EÄŸitim ve test veri boyutlarÄ±nÄ± gÃ¶relim
print("EÄŸitim veri boyutu:", X_train.shape)
print("Test veri boyutu:", X_test.shape)

# Lojistik regresyon modelini oluÅŸtur
model = LogisticRegression(max_iter=100)

# Modeli eÄŸit
model.fit(X_train, y_train)

# Test verisi ile tahmin yapalÄ±m
y_pred = model.predict(X_test)

# Ä°lk 10 tahmine bakalÄ±m
print("Tahminler:", y_pred[:10])
print("GerÃ§ek deÄŸerler:", y_test.values[:10])

# DoÄŸruluk oranÄ±nÄ± hesaplayalÄ±m
accuracy = accuracy_score(y_test, y_pred)
print("DoÄŸruluk (Accuracy):", accuracy)

# KarÄ±ÅŸÄ±klÄ±k matrisi
print("\nKarÄ±ÅŸÄ±klÄ±k Matrisi:")
print(confusion_matrix(y_test, y_pred))

# AyrÄ±ntÄ±lÄ± sÄ±nÄ±flandÄ±rma raporu
print("\nSÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(y_test, y_pred))

# Ã–zelliklerin Ã¶nemini gÃ¶relim
coeff_df = pd.DataFrame({
    "Feature": X.columns,
    "Coefficient": model.coef_[0]
}).sort_values(by="Coefficient", ascending=False)

print(coeff_df)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Ã–zellikler ve katsayÄ±lar (manuel girildi)
data = {
    "Feature": [
        "Polydipsia", "Polyuria", "Irritability", "Genital thrush", "partial paresis", "sudden weight loss",
        "visual blurring", "weakness", "Polyphagia", "Age", "Alopecia", "muscle stiffness", "Obesity",
        "delayed healing", "Itching", "Gender"
    ],
    "Coefficient": [
        2.634848, 2.578316, 1.470152, 1.336485, 1.014970, 0.903378,
        0.774093, 0.531706, 0.455958, -0.036840, -0.158542, -0.176877, -0.186113,
        -0.563419, -1.203638, -2.385263
    ]
}

# DataFrame oluÅŸtur ve sÄ±ralama yap
df_coef = pd.DataFrame(data)
df_coef_sorted = df_coef.sort_values(by="Coefficient", ascending=False)

# Grafik Ã§izimi
plt.figure(figsize=(10, 6))
sns.barplot(x="Coefficient", y="Feature", data=df_coef_sorted, palette="coolwarm")
plt.title("Lojistik Regresyon - Ã–zelliklerin KatsayÄ± Etkisi")
plt.xlabel("KatsayÄ± DeÄŸeri (Etki GÃ¼cÃ¼)")
plt.ylabel("Ã–zellik (Feature)")
plt.axvline(0, color="black", linestyle="--")
plt.grid(axis='x', linestyle='--', linewidth=0.5)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Confusion Matrix oluÅŸtur
cm = confusion_matrix(y_test, y_pred)

# Heatmap Ã§izelim
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Tahmin Edilen')
plt.ylabel('GerÃ§ek')
plt.title('Logistic Regresyon')
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Modelin tahminlerini alÄ±yoruz
y_pred_prob = model.predict_proba(X_test)[:, 1]  # EÄŸer model predict_proba kullanÄ±yorsa

# FPR ve TPR'yi hesapla
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

# AUC'yi hesapla
roc_auc = auc(fpr, tpr)

# ROC eÄŸrisini Ã§iz
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random model iÃ§in diagonal Ã§izgi
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""# Modelimizde Overfitting olup olmadÄ±ÄŸÄ±nÄ± kontrol edelim"""

# EÄŸer EÄŸitim > Test farkÄ± Ã§ok fazlaysa â†’ Overfitting olabilir.
train_acc = model.score(X_train, y_train)
test_acc = model.score(X_test, y_test)

print("EÄŸitim DoÄŸruluÄŸu:", train_acc)
print("Test DoÄŸruluÄŸu:", test_acc)
# DeÄŸerler yaklaÅŸÄ±k aynÄ± olduÄŸu iÃ§in Overfitting yok

"""ğŸ” **MÃ¼kemmel!** Hem eÄŸitim doÄŸruluÄŸun **%92.3**, hem test doÄŸruluÄŸun **%92.3** Ã§Ä±kmÄ±ÅŸ. Bu ÅŸu anlama gelir:

---

##  SonuÃ§: Overfitting Yok

  EÄŸitim ve test doÄŸruluÄŸu neredeyse eÅŸit** | Bu, modelin sadece eÄŸitim verisini ezberlemediÄŸini; yeni, gÃ¶rmediÄŸi verilerde de benzer baÅŸarÄ± gÃ¶sterdiÄŸini kanÄ±tlar. |
|  **AUC = 0.97** | Model gÃ¼Ã§lÃ¼, duyarlÄ± ve genelleyebilir durumda. |
|  **Yorum:** | Model **overfitting yapmÄ±yor**, aynÄ± zamanda **underfitting de deÄŸil** â€“ tam kararÄ±nda bir Ã¶ÄŸrenme dÃ¼zeyi var. |

---

##  Åu An Modelin Durumu:
- **Genelleme gÃ¼cÃ¼ yÃ¼ksek**
- **EzberlememiÅŸ**
- **Yeni verilere karÅŸÄ± saÄŸlam**
- GerÃ§ek dÃ¼nya uygulamalarÄ± iÃ§in oldukÃ§a uygun baÅŸlangÄ±Ã§ta

---

Ä°stersen ÅŸimdi:
- DiÄŸer algoritmalarla karÅŸÄ±laÅŸtÄ±ralÄ±m mÄ±?
- Yoksa model raporu, PDF veya sunum dosyasÄ± oluÅŸturalÄ±m mÄ±?

NasÄ±l ilerleyelim?

# Cross-Validation (Ã‡apraz DoÄŸrulama)
FarklÄ± veri dilimlerinde model kararlÄ± Ã§alÄ±ÅŸÄ±yor mu gÃ¶rmek iÃ§in yapÄ±lÄ±r. 
"""

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

# Modeli tanÄ±mla
model_cv = LogisticRegression(max_iter=1000)

# 5-Fold Cross-Validation ile doÄŸruluk hesapla
scores = cross_val_score(model_cv, X, y, cv=5, scoring='accuracy')

# SonuÃ§larÄ± yazdÄ±r
print("Her fold iÃ§in doÄŸruluk skorlarÄ±:", scores)
print("Ortalama doÄŸruluk:", scores.mean())
print("Standart sapma:", scores.std())

""" **MÃ¼kemmel bir sonuÃ§!** Cross-validation Ã§Ä±ktÄ±nÄ± deÄŸerlendirelim:
##  Cross-Validation SonuÃ§larÄ±

| Fold (kat) | DoÄŸruluk |
|------------|----------|
| Fold 1     | 0.8942   |
| Fold 2     | 0.9327   |
| Fold 3     | 0.8750   |
| Fold 4     | 0.9712   |
| Fold 5     | 0.9519   |

---

###  **Ortalama DoÄŸruluk: `0.925` (%92.5)**
- Bu, daha Ã¶nce hesapladÄ±ÄŸÄ±mÄ±z test doÄŸruluÄŸuyla (%92.3) neredeyse **bire bir uyuÅŸuyor**.
- Yani model **veri seti iÃ§inde tutarlÄ± Ã§alÄ±ÅŸÄ±yor** ve **ÅŸansa baÄŸlÄ± sapma gÃ¶stermiyor**
